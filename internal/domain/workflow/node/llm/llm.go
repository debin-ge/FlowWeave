package llm

import (
	applog "flowweave/internal/platform/log"
	"context"
	"encoding/json"
	"fmt"
	"sort"
	"strings"
	"sync"
	"time"

	"flowweave/internal/domain/memory"
	"flowweave/internal/domain/workflow/event"
	types "flowweave/internal/domain/workflow/model"
	"flowweave/internal/domain/workflow/node"
	"flowweave/internal/domain/workflow/port"
	"flowweave/internal/provider"
	"flowweave/internal/tool"
)

// LLMNodeData LLM èŠ‚ç‚¹é…ç½®æ•°æ®
type LLMNodeData struct {
	Type    string               `json:"type"`
	Title   string               `json:"title"`
	Model   ModelConfig          `json:"model"`
	Prompts []PromptTemplate     `json:"prompts"`
	Memory  *memory.MemoryConfig `json:"memory,omitempty"`
	Vision  *VisionConfig        `json:"vision,omitempty"`
	Tools   []ToolBinding        `json:"tools,omitempty"` // Agent å·¥å…·ç»‘å®šåˆ—è¡¨
}

// ToolBinding DSL ä¸­å•ä¸ªå·¥å…·çš„ç»‘å®šé…ç½®
type ToolBinding struct {
	Name        string                 `json:"name"`                  // å·¥å…·åç§°ï¼Œå¯¹åº” tool.Registry ä¸­çš„ key
	Description string                 `json:"description,omitempty"` // DSL å¯è¦†ç›–å·¥å…·æè¿°
	Args        map[string]interface{} `json:"args,omitempty"`        // é™æ€å‚æ•°ï¼ˆå¦‚ dataset_idsã€top_kï¼‰
}

// ModelConfig æ¨¡å‹é…ç½®
type ModelConfig struct {
	Provider    string  `json:"provider"` // openai, deepseek, etc.
	Name        string  `json:"name"`     // gpt-4, deepseek-chat, etc.
	Mode        string  `json:"mode"`     // chat, completion
	Temperature float64 `json:"temperature"`
	MaxTokens   int     `json:"max_tokens"`
	TopP        float64 `json:"top_p"`
}

// PromptTemplate æç¤ºè¯æ¨¡æ¿
type PromptTemplate struct {
	Role string `json:"role"` // system, user, assistant
	Text string `json:"text"` // æ”¯æŒ {{#node_id.var_name#}} æ¨¡æ¿å˜é‡
}

// VisionConfig è§†è§‰é…ç½®
type VisionConfig struct {
	Enabled bool `json:"enabled"`
}

// LLMNode LLM èŠ‚ç‚¹
type LLMNode struct {
	*node.BaseNode
	data LLMNodeData
}

func init() {
	node.Register(types.NodeTypeLLM, NewLLMNode)
}

// NewLLMNode åˆ›å»º LLM èŠ‚ç‚¹
func NewLLMNode(id string, rawData json.RawMessage) (node.Node, error) {
	var data LLMNodeData
	if err := json.Unmarshal(rawData, &data); err != nil {
		return nil, fmt.Errorf("parse LLM node data: %w", err)
	}

	// éªŒè¯è®°å¿†é…ç½®ä¾èµ–å…³ç³»
	if data.Memory != nil {
		if err := data.Memory.Validate(); err != nil {
			return nil, fmt.Errorf("invalid memory config: %w", err)
		}
	}

	// éªŒè¯å·¥å…·é…ç½®ï¼šdescription ç”± DSL æä¾›ï¼Œä¸èµ°ä»£ç å†…ç¡¬ç¼–ç 
	for i, tb := range data.Tools {
		if strings.TrimSpace(tb.Name) == "" {
			return nil, fmt.Errorf("invalid tool binding at index %d: name is required", i)
		}
		if strings.TrimSpace(tb.Description) == "" {
			return nil, fmt.Errorf("invalid tool binding %q: description is required in DSL", tb.Name)
		}
	}

	n := &LLMNode{
		BaseNode: node.NewBaseNode(id, types.NodeTypeLLM, data.Title, types.NodeExecutionTypeExecutable),
		data:     data,
	}
	return n, nil
}

// Run æ‰§è¡Œ LLM èŠ‚ç‚¹ï¼ˆæ”¯æŒ Agent Tool Calling å¾ªç¯ï¼‰
func (n *LLMNode) Run(ctx context.Context) (<-chan event.NodeEvent, error) {
	return node.RunStreamWithEvents(ctx, n, func(ctx context.Context, stream chan<- string) (*node.NodeRunResult, error) {
		// 1. è·å– LLM provider
		llmProvider, err := provider.GetProvider(n.data.Model.Provider)
		if err != nil {
			return nil, fmt.Errorf("get LLM provider: %w", err)
		}

		// 2. æ„å»ºæ¶ˆæ¯åˆ—è¡¨ï¼ˆè§£ææ¨¡æ¿å˜é‡ï¼‰
		vp, _ := node.GetVariablePoolFromContext(ctx)
		messages := n.buildMessages(vp)

		// 3. å¦‚æœå¯ç”¨äº†è®°å¿†ï¼Œåœ¨ prompt messages ä¸­æ’å…¥å†å²å¯¹è¯
		var userInput string
		messages, userInput = n.injectMemory(ctx, messages)

		// 4. æ„å»ºå·¥å…·å®šä¹‰ï¼ˆå¦‚æœ DSL é…ç½®äº† toolsï¼‰
		var toolDefs []provider.ToolDefinition
		var toolRegistry *tool.Registry
		if len(n.data.Tools) > 0 {
			toolRegistry, _ = tool.RegistryFromContext(ctx)
			if toolRegistry != nil {
				toolDefs = n.buildToolDefinitions(toolRegistry)
				names := make([]string, 0, len(toolDefs))
				for _, def := range toolDefs {
					names = append(names, def.Function.Name)
				}
				applog.Info("[LLM/Agent] Tools configured",
					"node_id", n.ID(),
					"tool_count", len(toolDefs),
					"tool_names", names,
				)
			}
		}

		// è®°å½•åˆå§‹è¯·æ±‚æ¶ˆæ¯ï¼ˆç”¨äºæº¯æºï¼‰
		traceMessages := make([]map[string]string, len(messages))
		for i, m := range messages {
			traceMessages[i] = map[string]string{
				"role":    m.Role,
				"content": m.Content,
			}
		}

		callStart := time.Now()

		// 5. Agent å¾ªç¯ï¼šå¦‚æœé…ç½®äº†å·¥å…·ï¼Œè¿›å…¥å¾ªç¯æ¨¡å¼
		var content string
		var totalTokens int
		const safetyLimit = 10

		if len(toolDefs) > 0 && toolRegistry != nil {
			// === Agent æ¨¡å¼ ===
			for round := 0; round < safetyLimit; round++ {
				req := &provider.CompletionRequest{
					Model:       n.data.Model.Name,
					Messages:    messages,
					Temperature: n.data.Model.Temperature,
					MaxTokens:   n.data.Model.MaxTokens,
					TopP:        n.data.Model.TopP,
					Tools:       toolDefs,
					ToolChoice:  "auto",
				}

				// éæµå¼è°ƒç”¨ï¼ˆä¸­é—´è½®æ¬¡ä¸éœ€è¦æµå¼è¾“å‡ºï¼‰
				resp, err := llmProvider.Complete(ctx, req)
				if err != nil {
					return nil, fmt.Errorf("LLM complete error (round %d): %w", round, err)
				}
				totalTokens += resp.Usage.TotalTokens

				// âœ… æ ¸å¿ƒåˆ¤æ–­ï¼šæ²¡æœ‰ tool_calls å°±æ˜¯æœ€ç»ˆç­”æ¡ˆ
				if len(resp.ToolCalls) == 0 {
					content = resp.Content
					// æµå¼è¾“å‡ºæœ€ç»ˆç­”æ¡ˆ
					if content != "" {
						stream <- content
					}
					applog.Info("[LLM/Agent] Final answer (no more tool calls)",
						"node_id", n.ID(),
						"rounds", round+1,
						"total_tokens", totalTokens,
					)
					break
				}

				// LLM è¿”å›äº† tool_callsï¼Œæ‰§è¡Œå·¥å…·
				applog.Info("[LLM/Agent] Tool calls received",
					"node_id", n.ID(),
					"round", round+1,
					"tool_count", len(resp.ToolCalls),
				)

				// è¿½åŠ  assistant æ¶ˆæ¯ï¼ˆæºå¸¦ tool_callsï¼‰
				messages = append(messages, provider.Message{
					Role:      "assistant",
					Content:   resp.Content,
					ToolCalls: resp.ToolCalls,
				})

				// å¹¶è¡Œæ‰§è¡Œå·¥å…·è°ƒç”¨ï¼ŒæŒ‰åŸå§‹é¡ºåºå›å¡« tool æ¶ˆæ¯
				toolMessages := make([]provider.Message, len(resp.ToolCalls))
				var toolWG sync.WaitGroup
				for i, tc := range resp.ToolCalls {
					i, tc := i, tc
					toolWG.Add(1)
					go func() {
						defer toolWG.Done()

						applog.Info("[LLM/Agent] Executing tool",
							"tool", tc.Function.Name,
							"call_id", tc.ID,
							"arguments", tc.Function.Arguments,
						)

						// åˆå¹¶ DSL é™æ€ args + LLM åŠ¨æ€ arguments
						execArgs := n.mergeToolArgs(tc.Function.Name, tc.Function.Arguments)

						toolResult, toolErr := toolRegistry.Execute(ctx, tc.Function.Name, execArgs)
						if toolErr != nil {
							toolResult = fmt.Sprintf("å·¥å…·æ‰§è¡Œå¤±è´¥: %s", toolErr.Error())
							applog.Error("[LLM/Agent] Tool execution failed",
								"tool", tc.Function.Name,
								"error", toolErr,
							)
						} else {
							resultPreview := toolResult
							if len(resultPreview) > 200 {
								resultPreview = resultPreview[:200] + "..."
							}
							applog.Info("[LLM/Agent] Tool result",
								"tool", tc.Function.Name,
								"result_preview", resultPreview,
							)
						}

						toolMessages[i] = provider.Message{
							Role:       "tool",
							Content:    toolResult,
							ToolCallID: tc.ID,
							Name:       tc.Function.Name,
						}
					}()
				}
				toolWG.Wait()
				messages = append(messages, toolMessages...)
				// ç»§ç»­å¾ªç¯ï¼Œå¸¦ç€å·¥å…·ç»“æœå†æ¬¡è°ƒç”¨ LLM
			}

			if content == "" {
				return nil, fmt.Errorf("exceeded safety limit (%d) for tool call rounds", safetyLimit)
			}
		} else {
			// === æ™®é€šæ¨¡å¼ï¼ˆæ— å·¥å…·ï¼Œç›´æ¥æµå¼è¾“å‡ºï¼‰ ===
			req := &provider.CompletionRequest{
				Model:       n.data.Model.Name,
				Messages:    messages,
				Temperature: n.data.Model.Temperature,
				MaxTokens:   n.data.Model.MaxTokens,
				TopP:        n.data.Model.TopP,
			}

			chunkCh, errCh := llmProvider.StreamComplete(ctx, req)

			var contentBuilder strings.Builder

		loop:
			for {
				select {
				case chunk, ok := <-chunkCh:
					if !ok {
						break loop
					}
					if chunk.Delta != "" {
						contentBuilder.WriteString(chunk.Delta)
						stream <- chunk.Delta
					}
				case err, ok := <-errCh:
					if ok && err != nil {
						return nil, fmt.Errorf("LLM stream error: %w", err)
					}
					if !ok {
						break loop
					}
				case <-ctx.Done():
					return nil, ctx.Err()
				}
			}

			content = contentBuilder.String()
		}

		callElapsed := time.Since(callStart).Milliseconds()

		// 6. å¦‚æœå¯ç”¨äº†è®°å¿†ï¼Œä¿å­˜æœ¬è½®å¯¹è¯
		n.saveMemory(ctx, userInput, content)

		return &node.NodeRunResult{
			Status: types.NodeExecutionStatusSucceeded,
			Outputs: map[string]interface{}{
				"text": content,
			},
			Metadata: map[string]interface{}{
				"provider":     n.data.Model.Provider,
				"model":        n.data.Model.Name,
				"total_tokens": totalTokens,
				"llm_trace": map[string]interface{}{
					"provider":    n.data.Model.Provider,
					"model":       n.data.Model.Name,
					"messages":    traceMessages,
					"temperature": n.data.Model.Temperature,
					"max_tokens":  n.data.Model.MaxTokens,
					"top_p":       n.data.Model.TopP,
					"response":    content,
					"elapsed_ms":  callElapsed,
				},
			},
		}, nil
	})
}

// buildToolDefinitions æ ¹æ® DSL tools æ„å»º provider å·¥å…·å®šä¹‰ã€‚
// description ç”± DSL æä¾›ï¼Œä¸å›é€€åˆ°å·¥å…·å®ç°ä¸­çš„é»˜è®¤å€¼ã€‚
func (n *LLMNode) buildToolDefinitions(reg *tool.Registry) []provider.ToolDefinition {
	defs := make([]provider.ToolDefinition, 0, len(n.data.Tools))
	for _, tb := range n.data.Tools {
		if tb.Name == "" {
			continue
		}

		t, ok := reg.Get(tb.Name)
		if !ok {
			applog.Warn("[LLM/Agent] Tool not found in registry",
				"node_id", n.ID(),
				"tool", tb.Name,
			)
			continue
		}

		defs = append(defs, provider.ToolDefinition{
			Type: "function",
			Function: provider.ToolFunction{
				Name:        t.Name(),
				Description: strings.TrimSpace(tb.Description),
				Parameters:  t.Parameters(),
			},
		})
	}
	return defs
}

// extractUserInput ä»æ¶ˆæ¯åˆ—è¡¨æå–å½“å‰ç”¨æˆ·è¾“å…¥ï¼ˆæœ€åä¸€æ¡é system æ¶ˆæ¯ï¼‰
func extractUserInput(messages []provider.Message) string {
	for i := len(messages) - 1; i >= 0; i-- {
		if messages[i].Role != "system" {
			return messages[i].Content
		}
	}
	return ""
}

// injectMemory å¦‚æœå¯ç”¨äº†è®°å¿†ï¼Œå°†å†å²å¯¹è¯æ’å…¥æ¶ˆæ¯åˆ—è¡¨
// ç»„è£…é¡ºåºï¼šsystem â†’ key_facts â†’ mid_term_summary â†’ gateway_summary â†’ recent_messages â†’ user_input
// è¿”å›ï¼šå®Œæ•´æ¶ˆæ¯åˆ—è¡¨ + å½“å‰ç”¨æˆ·è¾“å…¥æ–‡æœ¬
func (n *LLMNode) injectMemory(ctx context.Context, messages []provider.Message) ([]provider.Message, string) {
	// æ— è®ºæ˜¯å¦æ³¨å…¥è®°å¿†ï¼Œéƒ½å…ˆæå–å½“å‰ç”¨æˆ·è¾“å…¥ï¼ˆç”¨äºåç»­ saveMemoryï¼‰
	userInput := extractUserInput(messages)

	if n.data.Memory == nil || !n.data.Memory.IsShortTermEnabled() {
		applog.Debug("[LLM/Memory] Memory not enabled, skipping injection", "node_id", n.ID())
		return messages, userInput
	}

	coordinator, ok := memory.CoordinatorFromContext(ctx)
	if !ok {
		applog.Warn("[LLM/Memory] âš ï¸ No coordinator in context", "node_id", n.ID())
		return messages, userInput
	}

	convID := memory.ConversationIDFromContext(ctx)
	if convID == "" {
		applog.Warn("[LLM/Memory] âš ï¸ No conversation_id in context", "node_id", n.ID())
		return messages, userInput
	}

	applog.Info("[LLM/Memory] ğŸ”„ Starting memory injection",
		"node_id", n.ID(),
		"conversation_id", convID,
		"original_messages", len(messages),
		"current_user_input", userInput,
	)

	// å›å¿†å†å²
	recall, err := coordinator.Recall(ctx, &memory.RecallRequest{
		ConversationID: convID,
		Config:         n.data.Memory,
	})
	if err != nil {
		applog.Warn("[LLM/Memory] âš ï¸ Recall failed", "node_id", n.ID(), "error", err)
		return messages, userInput
	}

	// å¦‚æœæ²¡æœ‰ä»»ä½•è®°å¿†æ•°æ®ï¼Œç›´æ¥è¿”å›
	if len(recall.ShortTermMessages) == 0 && recall.MidTermSummary == "" &&
		recall.GatewaySummary == "" && len(recall.KeyFacts) == 0 {
		applog.Debug("[LLM/Memory] No memory data found, using original messages",
			"node_id", n.ID(),
			"conversation_id", convID,
		)
		return messages, userInput
	}

	// æå– system prompts
	var systemMsgs []provider.Message
	for _, msg := range messages {
		if msg.Role == "system" {
			systemMsgs = append(systemMsgs, msg)
		}
	}

	// ç»„è£…æ¶ˆæ¯é¡ºåºï¼šsystem â†’ key_facts â†’ mid_term_summary â†’ gateway_summary â†’ recent â†’ user_input
	result := make([]provider.Message, 0, len(systemMsgs)+len(recall.ShortTermMessages)+4)

	// 1. System prompts
	result = append(result, systemMsgs...)

	// 2. Key Factsï¼ˆå¦‚æœ‰ï¼‰
	if len(recall.KeyFacts) > 0 {
		factsContent := formatKeyFacts(recall.KeyFacts)
		result = append(result, provider.Message{
			Role:    "system",
			Content: "## å…³é”®äº‹å®\n" + factsContent,
		})
	}

	// 3. Mid-term Summaryï¼ˆé˜¶æ®µæ€§è®°å¿†æ‘˜è¦ï¼‰
	if recall.MidTermSummary != "" {
		result = append(result, provider.Message{
			Role:    "system",
			Content: "## é˜¶æ®µæ€§è®°å¿†æ‘˜è¦\n" + recall.MidTermSummary,
		})
	}

	// 4. Gateway Summaryï¼ˆè¿è¡Œæ—¶å‹ç¼©æ‘˜è¦ï¼‰
	if recall.GatewaySummary != "" {
		result = append(result, provider.Message{
			Role:    "system",
			Content: "## è¿è¡Œæ—¶å‹ç¼©æ‘˜è¦\n" + recall.GatewaySummary,
		})
	}

	// 5. Recent Messages
	result = append(result, recall.ShortTermMessages...)

	// 6. Current User Input
	if userInput != "" {
		result = append(result, provider.Message{Role: "user", Content: userInput})
	}

	userPreview := userInput
	if len(userPreview) > 100 {
		userPreview = userPreview[:100] + "..."
	}

	applog.Info("[LLM/Memory] âœ… Memory injected into messages",
		"node_id", n.ID(),
		"conversation_id", convID,
		"system_msgs", len(systemMsgs),
		"has_key_facts", len(recall.KeyFacts) > 0,
		"has_gateway_summary", recall.GatewaySummary != "",
		"has_mtm_summary", recall.MidTermSummary != "",
		"stm_history_msgs", len(recall.ShortTermMessages),
		"total_assembled_msgs", len(result),
		"current_user_input", userPreview,
	)

	// Debug: æ‰“å°å®Œæ•´æ¶ˆæ¯åˆ—è¡¨
	for i, msg := range result {
		contentPreview := msg.Content
		if len(contentPreview) > 80 {
			contentPreview = contentPreview[:80] + "..."
		}
		applog.Debug("[LLM/Memory] Assembled message",
			"index", i,
			"role", msg.Role,
			"content_preview", contentPreview,
		)
	}

	applog.Info("[LLM/Memory] ğŸ”„ Memory injection completed",
		"node_id", n.ID(),
		"conversation_id", convID,
		"total_assembled_msgs", len(result),
		"stm_history_msgs", result,
	)

	return result, userInput
}

// formatKeyFacts å°† Key Facts æ ¼å¼åŒ–ä¸ºå¯è¯»æ–‡æœ¬
func formatKeyFacts(facts map[string]string) string {
	if len(facts) == 0 {
		return ""
	}
	// æŒ‰ key å­—æ¯åºæ’åˆ—ä»¥ä¿è¯ç¨³å®šè¾“å‡º
	keys := make([]string, 0, len(facts))
	for k := range facts {
		keys = append(keys, k)
	}
	sort.Strings(keys)

	var sb strings.Builder
	for _, k := range keys {
		sb.WriteString(fmt.Sprintf("- %s: %s\n", k, facts[k]))
	}
	return sb.String()
}

// saveMemory ä¿å­˜æœ¬è½®å¯¹è¯åˆ°è®°å¿†
func (n *LLMNode) saveMemory(ctx context.Context, userInput, assistantOutput string) {
	if n.data.Memory == nil || !n.data.Memory.IsShortTermEnabled() {
		return
	}

	coordinator, ok := memory.CoordinatorFromContext(ctx)
	if !ok {
		applog.Warn("[LLM/Memory] âš ï¸ No coordinator for saveMemory", "node_id", n.ID())
		return
	}

	convID := memory.ConversationIDFromContext(ctx)
	if convID == "" {
		applog.Warn("[LLM/Memory] âš ï¸ No conversation_id for saveMemory", "node_id", n.ID())
		return
	}

	userPreview := userInput
	if len(userPreview) > 100 {
		userPreview = userPreview[:100] + "..."
	}
	assistantPreview := assistantOutput
	if len(assistantPreview) > 100 {
		assistantPreview = assistantPreview[:100] + "..."
	}

	applog.Info("[LLM/Memory] ğŸ’¾ Saving conversation turn",
		"node_id", n.ID(),
		"conversation_id", convID,
		"user_preview", userPreview,
		"assistant_preview", assistantPreview,
	)

	memReq := &memory.MemorizeRequest{
		ConversationID: convID,
		Config:         n.data.Memory,
		UserMessage:    provider.Message{Role: "user", Content: userInput},
		AssistantMsg:   provider.Message{Role: "assistant", Content: assistantOutput},
	}
	go func(req *memory.MemorizeRequest) {
		asyncCtx := context.Background()
		if orgID, tenantID, ok := port.RepoScopeFrom(ctx); ok {
			asyncCtx = port.WithRepoScope(asyncCtx, orgID, tenantID)
		}

		err := coordinator.Memorize(asyncCtx, req)
		if err != nil {
			applog.Warn("[LLM/Memory] âš ï¸ Memorize failed",
				"node_id", n.ID(),
				"conversation_id", convID,
				"error", err,
			)
		} else {
			applog.Info("[LLM/Memory] âœ… Conversation turn saved",
				"node_id", n.ID(),
				"conversation_id", convID,
			)
		}
	}(memReq)
}

// buildMessages ä»æ¨¡æ¿æ„å»ºæ¶ˆæ¯åˆ—è¡¨
func (n *LLMNode) buildMessages(vp node.VariablePoolAccessor) []provider.Message {
	messages := make([]provider.Message, 0, len(n.data.Prompts))

	for _, prompt := range n.data.Prompts {
		text := prompt.Text

		// è§£ææ¨¡æ¿å˜é‡ {{#node_id.var_name#}}
		if vp != nil {
			text = resolveTemplate(text, vp)
		}

		messages = append(messages, provider.Message{
			Role:    prompt.Role,
			Content: text,
		})
	}

	return messages
}

// resolveTemplate è§£ææ¨¡æ¿ä¸­çš„ {{#node_id.var_name#}} å¼•ç”¨
func resolveTemplate(template string, vp node.VariablePoolAccessor) string {
	var result strings.Builder
	i := 0
	for i < len(template) {
		if i+3 <= len(template) && template[i:i+3] == "{{#" {
			end := strings.Index(template[i+3:], "#}}")
			if end >= 0 {
				ref := template[i+3 : i+3+end]
				parts := strings.SplitN(ref, ".", 2)
				if len(parts) == 2 {
					val, ok := vp.GetVariable(types.VariableSelector(parts))
					if ok {
						result.WriteString(fmt.Sprintf("%v", val))
					}
				}
				i = i + 3 + end + 3
				continue
			}
		}
		result.WriteByte(template[i])
		i++
	}
	return result.String()
}

// mergeToolArgs åˆå¹¶ DSL é™æ€ Args å’Œ LLM åŠ¨æ€ Arguments
// DSL args ä½œä¸ºåŸºç¡€ï¼ŒLLM åŠ¨æ€å‚æ•°è¦†ç›–
func (n *LLMNode) mergeToolArgs(toolName string, llmArguments string) string {
	// æŸ¥æ‰¾ DSL ä¸­è¯¥å·¥å…·çš„é™æ€é…ç½®
	var dslArgs map[string]interface{}
	for _, tb := range n.data.Tools {
		if tb.Name == toolName {
			dslArgs = tb.Args
			break
		}
	}

	// å¦‚æœæ²¡æœ‰ DSL é™æ€å‚æ•°ï¼Œç›´æ¥è¿”å› LLM å‚æ•°
	if len(dslArgs) == 0 {
		return llmArguments
	}

	// è§£æ LLM åŠ¨æ€å‚æ•°
	var llmArgs map[string]interface{}
	if err := json.Unmarshal([]byte(llmArguments), &llmArgs); err != nil {
		// è§£æå¤±è´¥æ—¶ç›´æ¥è¿”å› LLM åŸå§‹å‚æ•°
		return llmArguments
	}

	// åˆå¹¶ï¼šDSL ä¸ºåŸºç¡€ï¼ŒLLM å‚æ•°ä¼˜å…ˆè¦†ç›–
	merged := make(map[string]interface{})
	for k, v := range dslArgs {
		merged[k] = v
	}
	for k, v := range llmArgs {
		merged[k] = v
	}

	result, err := json.Marshal(merged)
	if err != nil {
		return llmArguments
	}
	return string(result)
}
